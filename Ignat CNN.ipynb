{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["### Train/Test Split"],"metadata":{"id":"nthy312PrV3X"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"2DyUL4xlrLAq"},"outputs":[],"source":["# import packages\n","import numpy as np\n","import pandas as pd\n","import librosa\n","import librosa.display as dsp \n","import os \n","from tqdm import tqdm\n","import glob\n","import random\n","import collections\n","import shutil"]},{"cell_type":"code","source":["os.chdir('/Users/ignat/Documents/UC Davis/Fall 2022/ECS 271/Project')"],"metadata":{"id":"S0mnJo6FrZdh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# read in the list of the images\n","image_paths = []\n","path = 'spectogram_images'\n","for data_path in glob.glob(path + '/*/*'):\n","    image_paths.append(data_path)\n","\n","# shuffle the images\n","random.shuffle(image_paths)"],"metadata":{"id":"MIQEM8nYrZUi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def copy_img(img_paths, dest_folder):\n","  # double check the destination root folder exists\n","  if not os.path.exists(dest_folder):\n","    os.makedirs(dest_folder)\n","\n","  # move each image\n","  for img in tqdm(img_paths):\n","    img_name = img.split(os.path.sep)[-1]\n","    img_label = img.split(os.path.sep)[-2]\n","    label_folder = os.path.join(dest_folder, img_label)\n","\n","    # check if destination label folder exists \n","    if not os.path.exists(label_folder):\n","      os.makedirs(label_folder)\n","  \n","    # copy over the file\n","    destination = os.path.join(label_folder, img_name)\n","    shutil.copy(img, destination)"],"metadata":{"id":"xDRs9EfQrc9v"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def split_train_test(img_paths, train_dest_folder, test_dest_folder, split=0.8):\n","  # split into training and testing\n","  train_img_paths, test_img_paths = img_paths[:int(split*len(img_paths))], img_paths[int(split*len(img_paths)):] \n","  \n","  copy_img(train_img_paths, train_dest_folder)\n","  copy_img(test_img_paths, test_dest_folder)"],"metadata":{"id":"jxSTc5WCrfh4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# split images\n","split_train_test(image_paths, 'data/train', 'data/test', 0.8)"],"metadata":{"id":"JaNGE_VxrhxY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# check the distribution of digits in train and test\n","def check_dist(path):\n","    image_paths = []\n","    for data_path in glob.glob(path + '/*/*'):\n","        image_paths.append(data_path)\n","    \n","    counter = collections.Counter([x.split('/')[-2] for x in image_paths])\n","    return(counter)"],"metadata":{"id":"-147-h8yrkEm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Model Training"],"metadata":{"id":"5Vbc-es4rlFV"}},{"cell_type":"code","source":["import torchvision.transforms as transforms\n","import torchvision.datasets as datasets\n","from torch.utils.data import DataLoader\n","import os\n","import PIL\n","import numpy as np\n","import torch\n","import torchvision\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from tqdm import tqdm"],"metadata":{"id":"tI8eMk2grmUW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["os.chdir('/Users/ignat/Documents/UC Davis/Fall 2022/ECS 271/Project')"],"metadata":{"id":"-6KCgAR1rpnL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# batch size\n","BATCH_SIZE = 64"],"metadata":{"id":"T90J1GYhrsdC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# the training transforms\n","train_transform = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Resize((227,227)),\n","    transforms.Normalize(\n","        mean=[0.5, 0.5, 0.5],\n","        std=[0.5, 0.5, 0.5]\n","    )\n","])\n","# the validation transforms\n","valid_transform = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Resize((227,227)),\n","    transforms.Normalize(\n","        mean=[0.5, 0.5, 0.5],\n","        std=[0.5, 0.5, 0.5]\n","    )\n","])"],"metadata":{"id":"KbPoFT3zrt0x"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# training dataset\n","train_dataset = datasets.ImageFolder(\n","    root='data/train',\n","    transform=train_transform\n",")\n","\n","# training data loaders\n","train_loader = DataLoader(\n","    train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n","    num_workers=4, pin_memory=True\n",")"],"metadata":{"id":"XSOwic_DrvvF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# validation dataset\n","valid_dataset = datasets.ImageFolder(\n","    root='data/test',\n","    transform=valid_transform\n",")\n","\n","# validation data loaders\n","valid_loader = DataLoader(\n","    valid_dataset, batch_size=BATCH_SIZE, shuffle=False,\n","    num_workers=4, pin_memory=True\n",")"],"metadata":{"id":"K1y_ZQ-Mrxej"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class AlexNet(nn.Module):\n","    def __init__(self, num_classes=10):\n","        super(AlexNet, self).__init__()\n","        self.layer1 = nn.Sequential(\n","            nn.Conv2d(3, 96, kernel_size=11, stride=4, padding=0),\n","            nn.BatchNorm2d(96),\n","            nn.ReLU(),\n","            nn.MaxPool2d(kernel_size = 3, stride = 2))\n","        self.layer2 = nn.Sequential(\n","            nn.Conv2d(96, 256, kernel_size=5, stride=1, padding=2),\n","            nn.BatchNorm2d(256),\n","            nn.ReLU(),\n","            nn.MaxPool2d(kernel_size = 3, stride = 2))\n","        self.layer3 = nn.Sequential(\n","            nn.Conv2d(256, 384, kernel_size=3, stride=1, padding=1),\n","            nn.BatchNorm2d(384),\n","            nn.ReLU())\n","        self.layer4 = nn.Sequential(\n","            nn.Conv2d(384, 384, kernel_size=3, stride=1, padding=1),\n","            nn.BatchNorm2d(384),\n","            nn.ReLU())\n","        self.layer5 = nn.Sequential(\n","            nn.Conv2d(384, 256, kernel_size=3, stride=1, padding=1),\n","            nn.BatchNorm2d(256),\n","            nn.ReLU(),\n","            nn.MaxPool2d(kernel_size = 3, stride = 2))\n","        self.fc = nn.Sequential(\n","            nn.Dropout(0.5),\n","            nn.Linear(9216, 1024),\n","            nn.ReLU())\n","        self.fc1 = nn.Sequential(\n","            nn.Dropout(0.5),\n","            nn.Linear(1024, 1024),\n","            nn.ReLU())\n","        self.fc2= nn.Sequential(\n","            nn.Linear(1024, num_classes))\n","        \n","    def forward(self, x):\n","        out = self.layer1(x)\n","        out = self.layer2(out)\n","        out = self.layer3(out)\n","        out = self.layer4(out)\n","        out = self.layer5(out)\n","        out = out.reshape(out.size(0), -1)\n","        out = self.fc(out)\n","        out = self.fc1(out)\n","        out = self.fc2(out)\n","        return out"],"metadata":{"id":"Z4kdE70NrzN0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["num_classes = 10\n","num_epochs = 90\n","batch_size = 64\n","learning_rate = 0.001\n","\n","model = AlexNet(num_classes)\n","\n","# Loss and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay = 0.005, momentum = 0.9)  \n","\n","# Train the model\n","total_step = len(train_loader)"],"metadata":{"id":"1wG7El71r1IX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# helper functions\n","def save_model(epochs, model, optimizer, criterion):\n","    \"\"\"\n","    Function to save the trained model to disk.\n","    \"\"\"\n","    torch.save({\n","                'epoch': epochs,\n","                'model_state_dict': model.state_dict(),\n","                'optimizer_state_dict': optimizer.state_dict(),\n","                'loss': criterion,\n","                }, 'model.pth')"],"metadata":{"id":"kNy09bg1r2bz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# store loss\n","loss_tracker = []\n","\n","# store accuracy \n","test_acc_tracker = []\n","train_acc_tracker = []\n","\n","for epoch in range(num_epochs):\n","    running_loss = 0.0\n","    for i, (images, labels) in enumerate(train_loader):  \n","        # Move tensors to the configured device\n","        images = images\n","        labels = labels\n","        \n","        # Forward pass\n","        outputs = model(images)\n","        loss = criterion(outputs, labels)\n","        \n","        # Backward and optimize\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        running_loss += loss.item()\n","\n","    print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n","                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n","    loss_tracker.append(running_loss)\n","\n","    # Validation\n","    with torch.no_grad():\n","        correct = 0\n","        total = 0\n","        for images, labels in valid_loader:\n","            images = images\n","            labels = labels\n","            outputs = model(images)\n","            _, predicted = torch.max(outputs.data, 1)\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","            del images, labels, outputs\n","\n","    #print('Accuracy on validation images: {} %'.format( 100 * correct / total)) \n","    test_acc_tracker.append(correct / total)\n","\n","    # Testing set accuracy\n","    with torch.no_grad():\n","        correct = 0\n","        total = 0\n","        for images, labels in train_loader:\n","            images = images\n","            labels = labels\n","            outputs = model(images)\n","            _, predicted = torch.max(outputs.data, 1)\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","            del images, labels, outputs\n","\n","    train_acc_tracker.append(correct / total)\n","\n","    # save model\n","    save_model(epoch, model, optimizer, criterion)\n","\n","# "],"metadata":{"id":"smtuEhTir4-L"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# loss\n","plt.plot(np.arange(0,len(loss_tracker)), loss_tracker)\n","plt.title('Loss Over the Epochs')\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss')"],"metadata":{"id":"5Vl6zEyer7lC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# accuracy\n","plt.plot(np.arange(0,len(test_acc_tracker)), test_acc_tracker)\n","plt.title('Testing Accuracy Over the Epochs')\n","plt.xlabel('Epoch')\n","plt.ylabel('% Accuracy')"],"metadata":{"id":"q-V4twRIr9ci"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# get all ground truths and predicted labels\n","with torch.no_grad():\n","    y_pred = []\n","    y_true = []\n","    for images, labels in tqdm(valid_loader):\n","        labels = labels\n","        outputs = model(images)\n","        _, predicted = torch.max(outputs.data, 1)\n","        y_pred.append(predicted.tolist())\n","        y_true.append(labels.tolist())\n"],"metadata":{"id":"mK1msDs6sBmB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y_true = [item for sublist in y_true for item in sublist]\n","y_pred = [item for sublist in y_pred for item in sublist]"],"metadata":{"id":"-I71dGW6sDgt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["target_names = [str(x) for x in range(10)]\n","output_dict = classification_report(y_true, y_pred, output_dict = True)\n","df_temp = pd.DataFrame(output_dict).transpose()\n","df_temp"],"metadata":{"id":"f3UzUHgIsFSr"},"execution_count":null,"outputs":[]}]}